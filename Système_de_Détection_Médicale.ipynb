{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOKEZTUnKwypLLb0DLBmK0X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/martialaristide/AGRICAM-IA-/blob/main/Syst%C3%A8me_de_D%C3%A9tection_M%C3%A9dicale.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "0RHB0Kuw6LRg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import json\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms, models\n",
        "from torchvision.utils import make_grid\n",
        "import torchvision.transforms.functional as TF"
      ],
      "metadata": {
        "id": "TyFP2sVM60f3"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pydicom\n",
        "import nibabel as nib\n",
        "\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\" Toutes les biblioth√®ques import√©es avec succ√®s!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6qwzlnm7Lsh",
        "outputId": "e9ca8bb4-da79-40ca-dfb9-722d185f730b"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Toutes les biblioth√®ques import√©es avec succ√®s!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydicom\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SD7ZumaL9n3n",
        "outputId": "7b5dccd8-496c-4757-8b6a-f041482def28"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pydicom in /usr/local/lib/python3.12/dist-packages (3.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pydicom\n",
        "import nibabel as nib"
      ],
      "metadata": {
        "id": "xf2g_-8g907a"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "fzIg6jcb944x"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\" Toutes les biblioth√®ques import√©es avec succ√®s!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYaWAJ6F-AEY",
        "outputId": "b1100e71-bd69-4172-e303-1bb345a959b2"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Toutes les biblioth√®ques import√©es avec succ√®s!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MedicalDataLoader:\n",
        "    \"\"\"\n",
        "    Classe pour charger et pr√©parer les donn√©es m√©dicales\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data_path):\n",
        "        self.data_path = data_path\n",
        "        self.supported_formats = ['.dcm', '.nii', '.nii.gz', '.png', '.jpg']\n",
        "\n",
        "    def load_dicom(self, file_path):\n",
        "        \"\"\"Charge un fichier DICOM (radiographies)\"\"\"\n",
        "        try:\n",
        "            dicom = pydicom.dcmread(file_path)\n",
        "            image = dicom.pixel_array\n",
        "            image = image.astype(np.float32) / np.max(image)\n",
        "            return image, dicom\n",
        "        except Exception as e:\n",
        "            print(f\"Erreur lecture DICOM {file_path}: {e}\")\n",
        "            return None, None\n",
        "\n",
        "    def load_nifti(self, file_path):\n",
        "        \"\"\"Charge un fichier NIFTI (IRM)\"\"\"\n",
        "        try:\n",
        "            nifti = nib.load(file_path)\n",
        "            image = nifti.get_fdata()\n",
        "            return image, nifti\n",
        "        except Exception as e:\n",
        "            print(f\"Erreur lecture NIFTI {file_path}: {e}\")\n",
        "            return None, None\n",
        "\n",
        "    def explore_dataset(self):\n",
        "        \"\"\"Explore la structure du dataset\"\"\"\n",
        "        print(\" Exploration du dataset m√©dical...\")\n",
        "\n",
        "        file_types = {}\n",
        "        for root, dirs, files in os.walk(self.data_path):\n",
        "            for file in files:\n",
        "                ext = os.path.splitext(file)[1].lower()\n",
        "                if ext in self.supported_formats:\n",
        "                    file_types[ext] = file_types.get(ext, 0) + 1\n",
        "\n",
        "        print(\" Distribution des fichiers:\", file_types)\n",
        "\n",
        "\n",
        "        self.preview_medical_images()\n",
        "\n",
        "    def preview_medical_images(self):\n",
        "        \"\"\"Placeholder for previewing medical images - functionality to be implemented\"\"\"\n",
        "        print(\" Previewing medical images (functionality to be implemented)...\")\n",
        "\n",
        "data_loader = MedicalDataLoader('./medical_data/')\n",
        "data_loader.explore_dataset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o54cix93-JKe",
        "outputId": "1f6a8245-eba4-4adc-ee1d-509de7400c58"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Exploration du dataset m√©dical...\n",
            " Distribution des fichiers: {}\n",
            " Previewing medical images (functionality to be implemented)...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MedicalClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    CNN pour classification de pathologies sur images radiologiques\n",
        "    Utilise transfer learning avec ResNet50\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes=2, pretrained=True):\n",
        "        super(MedicalClassifier, self).__init__()\n",
        "\n",
        "        self.backbone = models.resnet50(pretrained=pretrained)\n",
        "\n",
        "        in_features = self.backbone.fc.in_features\n",
        "        self.backbone.fc = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(in_features, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.backbone(x)\n",
        "\n",
        "print(\" Mod√®le de classification cr√©√©!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C69yi8X2_gYz",
        "outputId": "9dd9c4af-6366-4401-9d62-e0fdc799d975"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Mod√®le de classification cr√©√©!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class UNetSegmentation(nn.Module):\n",
        "    \"\"\"\n",
        "    Architecture U-Net pour segmentation de tumeurs en IRM\n",
        "    Sp√©cialement con√ßu pour l'imagerie m√©dicale\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels=1, out_channels=1, features=[64, 128, 256, 512]):\n",
        "        super(UNetSegmentation, self).__init__()\n",
        "\n",
        "        self.encoder = nn.ModuleList()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        for feature in features:\n",
        "            self.encoder.append(self._double_conv(in_channels, feature))\n",
        "            in_channels = feature\n",
        "\n",
        "        self.bottleneck = self._double_conv(features[-1], features[-1] * 2)\n",
        "\n",
        "        self.decoder = nn.ModuleList()\n",
        "        self.upconvs = nn.ModuleList()\n",
        "\n",
        "        for feature in reversed(features):\n",
        "            self.upconvs.append(\n",
        "                nn.ConvTranspose2d(feature * 2, feature, kernel_size=2, stride=2)\n",
        "            )\n",
        "            self.decoder.append(self._double_conv(feature * 2, feature))\n",
        "\n",
        "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
        "\n",
        "    def _double_conv(self, in_channels, out_channels):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        skip_connections = []\n",
        "\n",
        "        for down in self.encoder:\n",
        "            x = down(x)\n",
        "            skip_connections.append(x)\n",
        "            x = self.pool(x)\n",
        "\n",
        "        x = self.bottleneck(x)\n",
        "        skip_connections = skip_connections[::-1]\n",
        "\n",
        "        for idx in range(len(self.decoder)):\n",
        "            x = self.upconvs[idx](x)\n",
        "            skip_connection = skip_connections[idx]\n",
        "\n",
        "\n",
        "            if x.shape != skip_connection.shape:\n",
        "                x = TF.resize(x, size=skip_connection.shape[2:])\n",
        "\n",
        "            concat_skip = torch.cat((skip_connection, x), dim=1)\n",
        "            x = self.decoder[idx](concat_skip)\n",
        "\n",
        "        return torch.sigmoid(self.final_conv(x))\n",
        "\n",
        "print(\" Mod√®le de segmentation U-Net cr√©√©!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_tboByQAA-2",
        "outputId": "3c4c3e35-b103-4db2-82a6-abcbf27bfbcc"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Mod√®le de segmentation U-Net cr√©√©!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MedicalDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset personnalis√© pour donn√©es m√©dicales\n",
        "    G√®re DICOM, NIFTI et formats standards\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, image_paths, masks_paths=None, transform=None,\n",
        "                 mode='classification', image_size=256):\n",
        "        self.image_paths = image_paths\n",
        "        self.masks_paths = masks_paths\n",
        "        self.transform = transform\n",
        "        self.mode = mode\n",
        "        self.image_size = image_size\n",
        "        self.data_loader = MedicalDataLoader('')\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        image = self.load_medical_image(img_path)\n",
        "\n",
        "        if self.mode == 'classification':\n",
        "            label = self.get_label_from_path(img_path)\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "            return image, label\n",
        "\n",
        "        elif self.mode == 'segmentation':\n",
        "            mask_path = self.masks_paths[idx]\n",
        "            mask = self.load_medical_image(mask_path)\n",
        "\n",
        "            if self.transform:\n",
        "                augmented = self.transform(image=image, mask=mask)\n",
        "                image, mask = augmented['image'], augmented['mask']\n",
        "\n",
        "            return image, mask\n",
        "\n",
        "    def load_medical_image(self, path):\n",
        "        \"\"\"Charge une image m√©dicale selon son format\"\"\"\n",
        "        ext = os.path.splitext(path)[1].lower()\n",
        "\n",
        "        if ext == '.dcm':\n",
        "            image, _ = self.data_loader.load_dicom(path)\n",
        "        elif ext in ['.nii', '.nii.gz']:\n",
        "            image, _ = self.data_loader.load_nifti(path)\n",
        "        else:\n",
        "            image = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
        "            image = image.astype(np.float32) / 255.0\n",
        "\n",
        "        if len(image.shape) == 3:\n",
        "            image = image[:, :, image.shape[2]//2]\n",
        "\n",
        "        image = cv2.resize(image, (self.image_size, self.image_size))\n",
        "        return image\n",
        "\n",
        "    def get_label_from_path(self, path):\n",
        "        \"\"\"Extrait le label du chemin (√† adapter selon la structure)\"\"\"\n",
        "        if 'normal' in path.lower():\n",
        "            return 0\n",
        "        elif 'cancer' in path.lower() or 'tumor' in path.lower():\n",
        "            return 1\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "print(\" Dataset m√©dical cr√©√©!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6r-zKo1wAcdF",
        "outputId": "506ff746-1f95-44a1-97ad-9c8d8535db38"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Dataset m√©dical cr√©√©!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MedicalTrainer:\n",
        "    \"\"\"\n",
        "    Classe pour l'entra√Ænement des mod√®les m√©dicaux\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, device, model_type='classification'):\n",
        "        self.model = model\n",
        "        self.device = device\n",
        "        self.model_type = model_type\n",
        "        self.history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
        "\n",
        "    def train_classification(self, train_loader, val_loader, criterion,\n",
        "                           optimizer, scheduler, epochs=100):\n",
        "        \"\"\"\n",
        "        Entra√Ænement pour la classification\n",
        "        \"\"\"\n",
        "        print(\" D√©but de l'entra√Ænement classification...\")\n",
        "\n",
        "        best_val_acc = 0.0\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            self.model.train()\n",
        "            train_loss = 0.0\n",
        "            train_correct = 0\n",
        "            train_total = 0\n",
        "\n",
        "            for images, labels in tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}'):\n",
        "                images, labels = images.to(self.device), labels.to(self.device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                outputs = self.model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                train_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                train_total += labels.size(0)\n",
        "                train_correct += (predicted == labels).sum().item()\n",
        "\n",
        "            val_loss, val_acc = self.validate_classification(val_loader, criterion)\n",
        "\n",
        "            self.history['train_loss'].append(train_loss/len(train_loader))\n",
        "            self.history['val_loss'].append(val_loss)\n",
        "            self.history['train_acc'].append(100.*train_correct/train_total)\n",
        "            self.history['val_acc'].append(val_acc)\n",
        "\n",
        "            if scheduler:\n",
        "                scheduler.step()\n",
        "\n",
        "            if val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "                torch.save(self.model.state_dict(), 'best_classification_model.pth')\n",
        "\n",
        "            print(f'Epoch {epoch+1}/{epochs}:')\n",
        "            print(f'  Train Loss: {train_loss/len(train_loader):.4f}, Acc: {100.*train_correct/train_total:.2f}%')\n",
        "            print(f'  Val Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%')\n",
        "\n",
        "    def validate_classification(self, val_loader, criterion):\n",
        "        \"\"\"Validation pour la classification\"\"\"\n",
        "        self.model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(self.device), labels.to(self.device)\n",
        "                outputs = self.model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        return val_loss/len(val_loader), 100.*val_correct/val_total\n",
        "\n",
        "print(\" Trainer m√©dical cr√©√©!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VaNjKV_BCcE",
        "outputId": "a4af8750-26ee-4253-b695-b56c0d183bb5"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Trainer m√©dical cr√©√©!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MedicalEvaluator:\n",
        "    \"\"\"\n",
        "    √âvaluation sp√©cifique pour mod√®les m√©dicaux\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, device):\n",
        "        self.model = model\n",
        "        self.device = device\n",
        "\n",
        "    def evaluate_classification(self, test_loader):\n",
        "        \"\"\"√âvaluation compl√®te pour la classification\"\"\"\n",
        "        self.model.eval()\n",
        "        all_predictions = []\n",
        "        all_labels = []\n",
        "        all_probabilities = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, labels in test_loader:\n",
        "                images = images.to(self.device)\n",
        "                outputs = self.model(images)\n",
        "                probabilities = F.softmax(outputs, dim=1)\n",
        "                _, predictions = torch.max(outputs, 1)\n",
        "\n",
        "                all_predictions.extend(predictions.cpu().numpy())\n",
        "                all_labels.extend(labels.numpy())\n",
        "                all_probabilities.extend(probabilities.cpu().numpy())\n",
        "\n",
        "        self.calculate_medical_metrics(all_labels, all_predictions, all_probabilities)\n",
        "\n",
        "    def calculate_medical_metrics(self, true_labels, predictions, probabilities):\n",
        "        \"\"\"Calcule les m√©triques sp√©cifiques au domaine m√©dical\"\"\"\n",
        "\n",
        "        print(\" Classification Report:\")\n",
        "        print(classification_report(true_labels, predictions, target_names=['Normal', 'Pathologique']))\n",
        "\n",
        "        cm = confusion_matrix(true_labels, predictions)\n",
        "        tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "        sensitivity = tp / (tp + fn)\n",
        "        specificity = tn / (tn + fp)\n",
        "        precision = tp / (tp + fp)\n",
        "        f1_score = 2 * (precision * sensitivity) / (precision + sensitivity)\n",
        "\n",
        "        auc_roc = roc_auc_score(true_labels, [p[1] for p in probabilities])\n",
        "\n",
        "        print(\"\\nüè• M√©triques M√©dicales:\")\n",
        "        print(f\"Sensibilit√© (Recall positif): {sensitivity:.4f}\")\n",
        "        print(f\"Sp√©cificit√© (Recall n√©gatif): {specificity:.4f}\")\n",
        "        print(f\"Pr√©cision: {precision:.4f}\")\n",
        "        print(f\"F1-Score: {f1_score:.4f}\")\n",
        "        print(f\"AUC-ROC: {auc_roc:.4f}\")\n",
        "\n",
        "        self.plot_confusion_matrix(cm)\n",
        "        self.plot_roc_curve(true_labels, probabilities)\n",
        "\n",
        "    def plot_confusion_matrix(self, cm):\n",
        "        \"\"\"Plot la matrice de confusion\"\"\"\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                   xticklabels=['Normal', 'Pathologique'],\n",
        "                   yticklabels=['Normal', 'Pathologique'])\n",
        "        plt.title('Matrice de Confusion - D√©tection M√©dicale')\n",
        "        plt.ylabel('Vrai label')\n",
        "        plt.xlabel('Pr√©diction')\n",
        "        plt.show()\n",
        "\n",
        "print(\" √âvaluateur m√©dical cr√©√©!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ab_C8UCfBnGz",
        "outputId": "0c10d8c5-eb97-4aa2-d5c4-c0ee47de4959"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " √âvaluateur m√©dical cr√©√©!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"\n",
        "    Pipeline complet du syst√®me de d√©tection m√©dicale\n",
        "    \"\"\"\n",
        "    print(\" LANCEMENT DU SYST√àME DE D√âTECTION M√âDICALE IA\")\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\" Device utilis√©: {device}\")\n",
        "\n",
        "    print(\"\\n1.  Pr√©paration des donn√©es...\")\n",
        "\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomRotation(10),\n",
        "        transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "    ])\n",
        "\n",
        "    val_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "    ])\n",
        "\n",
        "    print(\"\\n2.  Initialisation des mod√®les...\")\n",
        "\n",
        "    classification_model = MedicalClassifier(num_classes=2).to(device)\n",
        "\n",
        "    segmentation_model = UNetSegmentation(in_channels=1, out_channels=1).to(device)\n",
        "\n",
        "    print(f\" Mod√®le classification: {sum(p.numel() for p in classification_model.parameters()):,} param√®tres\")\n",
        "    print(f\" Mod√®le segmentation: {sum(p.numel() for p in segmentation_model.parameters()):,} param√®tres\")\n",
        "\n",
        "    print(\"\\n3.  Configuration de l'entra√Ænement...\")\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(classification_model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
        "\n",
        "    print(\"\\n4.  Entra√Ænement des mod√®les...\")\n",
        "\n",
        "    print(\"\\n5.  √âvaluation des mod√®les...\")\n",
        "\n",
        "    print(\"\\n SYST√àME M√âDICAL IA PR√äT √Ä L'EMPLOI!\")\n",
        "    print(\"\\n Mes prochaines √©tapes:\")\n",
        "    print(\"   - Charger vos donn√©es m√©dicales DICOM/NIFTI\")\n",
        "    print(\"   - Adapter les paths dans MedicalDataset\")\n",
        "    print(\"   - Lancer l'entra√Ænement complet\")\n",
        "    print(\"   - D√©ployer le mod√®le avec une interface web\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rkv-giqOCHv3",
        "outputId": "8996f4b1-6a65-4c55-b465-9350a12acfaa"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " LANCEMENT DU SYST√àME DE D√âTECTION M√âDICALE IA\n",
            " Device utilis√©: cpu\n",
            "\n",
            "1.  Pr√©paration des donn√©es...\n",
            "\n",
            "2.  Initialisation des mod√®les...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Mod√®le classification: 24,558,146 param√®tres\n",
            " Mod√®le segmentation: 31,036,481 param√®tres\n",
            "\n",
            "3.  Configuration de l'entra√Ænement...\n",
            "\n",
            "4.  Entra√Ænement des mod√®les...\n",
            "\n",
            "5.  √âvaluation des mod√®les...\n",
            "\n",
            " SYST√àME M√âDICAL IA PR√äT √Ä L'EMPLOI!\n",
            "\n",
            " Mes prochaines √©tapes:\n",
            "   - Charger vos donn√©es m√©dicales DICOM/NIFTI\n",
            "   - Adapter les paths dans MedicalDataset\n",
            "   - Lancer l'entra√Ænement complet\n",
            "   - D√©ployer le mod√®le avec une interface web\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "try:\n",
        "    import pydicom\n",
        "    print(\" pydicom import√© avec succ√®s\")\n",
        "except ImportError:\n",
        "    print(\" pydicom non install√©. Ex√©cutez: pip install pydicom\")\n",
        "    pydicom = None\n",
        "\n",
        "try:\n",
        "    import nibabel as nib\n",
        "    print(\" nibabel import√© avec succ√®s\")\n",
        "except ImportError:\n",
        "    print(\" nibabel non install√©. Ex√©cutez: pip install nibabel\")\n",
        "    nib = None\n",
        "\n",
        "class MedicalDataLoader:\n",
        "    \"\"\"\n",
        "    Chargeur professionnel pour donn√©es m√©dicales DICOM et NIFTI\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, base_path=\"./medical_data\"):\n",
        "        self.base_path = base_path\n",
        "        self.dicom_data = []\n",
        "        self.nifti_data = []\n",
        "\n",
        "    def scan_medical_files(self):\n",
        "        \"\"\"Scan le r√©pertoire √† la recherche de fichiers m√©dicaux\"\"\"\n",
        "        print(\" Scan des fichiers m√©dicaux...\")\n",
        "\n",
        "        dicom_patterns = [\n",
        "            \"**/*.dcm\",\n",
        "            \"**/*.DCM\",\n",
        "            \"**/*.dicom\",\n",
        "            \"**/*.DICOM\"\n",
        "        ]\n",
        "\n",
        "        nifti_patterns = [\n",
        "            \"**/*.nii\",\n",
        "            \"**/*.nii.gz\",\n",
        "            \"**/*.NII\",\n",
        "            \"**/*.NII.GZ\"\n",
        "        ]\n",
        "\n",
        "        for pattern in dicom_patterns:\n",
        "            files = glob.glob(os.path.join(self.base_path, pattern), recursive=True)\n",
        "            self.dicom_data.extend([(f, self._infer_dicom_label(f)) for f in files])\n",
        "\n",
        "        for pattern in nifti_patterns:\n",
        "            files = glob.glob(os.path.join(self.base_path, pattern), recursive=True)\n",
        "            self.nifti_data.extend([(f, self._infer_nifti_label(f)) for f in files])\n",
        "\n",
        "        print(f\" Fichiers DICOM trouv√©s: {len(self.dicom_data)}\")\n",
        "        print(f\" Fichiers NIFTI trouv√©s: {len(self.nifti_data)}\")\n",
        "\n",
        "    def _infer_dicom_label(self, file_path):\n",
        "        \"\"\"Inf√®re le label √† partir du chemin du fichier DICOM\"\"\"\n",
        "        path_lower = file_path.lower()\n",
        "        if 'normal' in path_lower or 'sain' in path_lower:\n",
        "            return 0\n",
        "        elif 'cancer' in path_lower or 'tumeur' in path_lower or 'pathological' in path_lower:\n",
        "            return 1\n",
        "        elif 'covid' in path_lower or 'pneumonia' in path_lower:\n",
        "            return 2\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "    def _infer_nifti_label(self, file_path):\n",
        "        \"\"\"Inf√®re le label √† partir du chemin du fichier NIFTI\"\"\"\n",
        "        path_lower = file_path.lower()\n",
        "        if 'mask' in path_lower:\n",
        "            return 'mask'\n",
        "        elif 'tumor' in path_lower or 'lesion' in path_lower:\n",
        "            return 'tumor'\n",
        "        else:\n",
        "            return 'image'\n",
        "\n",
        "    def load_dicom_series(self, folder_path):\n",
        "        \"\"\"Charge une s√©rie compl√®te DICOM\"\"\"\n",
        "        if not pydicom:\n",
        "            print(\" pydicom non disponible\")\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            dicom_files = []\n",
        "            for file in os.listdir(folder_path):\n",
        "                if file.lower().endswith(('.dcm', '.dicom')):\n",
        "                    file_path = os.path.join(folder_path, file)\n",
        "                    dicom = pydicom.dcmread(file_path)\n",
        "                    dicom_files.append(dicom)\n",
        "\n",
        "            dicom_files.sort(key=lambda x: float(x.ImagePositionPatient[2]))\n",
        "\n",
        "            volume = np.stack([d.pixel_array for d in dicom_files])\n",
        "\n",
        "            print(f\" S√©rie DICOM charg√©e: {volume.shape}\")\n",
        "            return volume\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" Erreur chargement s√©rie DICOM: {e}\")\n",
        "            return None\n",
        "\n",
        "    def load_dicom_image(self, file_path):\n",
        "        \"\"\"Charge une image DICOM individuelle\"\"\"\n",
        "        if not pydicom:\n",
        "            print(\" pydicom non disponible\")\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            dicom = pydicom.dcmread(file_path)\n",
        "            metadata = {\n",
        "                'patient_id': getattr(dicom, 'PatientID', 'Unknown'),\n",
        "                'study_date': getattr(dicom, 'StudyDate', 'Unknown'),\n",
        "                'modality': getattr(dicom, 'Modality', 'Unknown'),\n",
        "                'body_part': getattr(dicom, 'BodyPartExamined', 'Unknown'),\n",
        "                'pixel_spacing': getattr(dicom, 'PixelSpacing', [1.0, 1.0])\n",
        "            }\n",
        "\n",
        "            image = dicom.pixel_array.astype(np.float32)\n",
        "\n",
        "            if dicom.PhotometricInterpretation == \"MONOCHROME1\":\n",
        "                image = np.max(image) - image\n",
        "\n",
        "            image = (image - np.min(image)) / (np.max(image) - np.min(image) + 1e-8)\n",
        "\n",
        "            return image, metadata\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" Erreur chargement DICOM {file_path}: {e}\")\n",
        "            return None, None\n",
        "\n",
        "    def load_nifti_volume(self, file_path):\n",
        "        \"\"\"Charge un volume NIFTI (IRM, CT scan)\"\"\"\n",
        "        if not nib:\n",
        "            print(\" nibabel non disponible\")\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            nifti = nib.load(file_path)\n",
        "            volume = nifti.get_fdata()\n",
        "            affine = nifti.affine\n",
        "            header = nifti.header\n",
        "\n",
        "            metadata = {\n",
        "                'shape': volume.shape,\n",
        "                'data_type': header.get_data_dtype(),\n",
        "                'voxel_sizes': header.get_zooms(),\n",
        "                'affine': affine\n",
        "            }\n",
        "\n",
        "            print(f\" NIFTI charg√©: {volume.shape}, type: {volume.dtype}\")\n",
        "            return volume, metadata\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" Erreur chargement NIFTI {file_path}: {e}\")\n",
        "            return None, None\n",
        "\n",
        "    def preprocess_medical_image(self, image, target_size=(256, 256)):\n",
        "        \"\"\"Pr√©traitement standard pour images m√©dicales\"\"\"\n",
        "        import cv2\n",
        "\n",
        "        if len(image.shape) == 3:\n",
        "            processed = np.zeros((image.shape[0], target_size[0], target_size[1]))\n",
        "            for i in range(image.shape[0]):\n",
        "                slice_img = cv2.resize(image[i], target_size)\n",
        "                processed[i] = slice_img\n",
        "        else:\n",
        "            processed = cv2.resize(image, target_size)\n",
        "\n",
        "        processed = np.clip(processed, 0, 1)\n",
        "\n",
        "        return processed\n",
        "\n",
        "    def visualize_medical_data(self, max_samples=5):\n",
        "        \"\"\"Visualise les donn√©es m√©dicales charg√©es\"\"\"\n",
        "        fig, axes = plt.subplots(2, max_samples, figsize=(15, 6))\n",
        "\n",
        "\n",
        "        print(\"  Visualisation des donn√©es DICOM...\")\n",
        "        for i, (dicom_path, label) in enumerate(self.dicom_data[:max_samples]):\n",
        "            image, metadata = self.load_dicom_image(dicom_path)\n",
        "            if image is not None:\n",
        "                axes[0, i].imshow(image, cmap='gray')\n",
        "                axes[0, i].set_title(f'DICOM: {label}\\n{metadata[\"modality\"]}')\n",
        "                axes[0, i].axis('off')\n",
        "\n",
        "        print(\"  Visualisation des donn√©es NIFTI...\")\n",
        "        nifti_samples = [x for x in self.nifti_data if x[1] != 'mask'][:max_samples]\n",
        "        for i, (nifti_path, label) in enumerate(nifti_samples):\n",
        "            volume, metadata = self.load_nifti_volume(nifti_path)\n",
        "            if volume is not None:\n",
        "                slice_idx = volume.shape[2] // 2 if len(volume.shape) == 3 else 0\n",
        "                slice_img = volume[:, :, slice_idx] if len(volume.shape) == 3 else volume\n",
        "\n",
        "                axes[1, i].imshow(slice_img, cmap='gray')\n",
        "                axes[1, i].set_title(f'NIFTI: {label}\\n{volume.shape}')\n",
        "                axes[1, i].axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "def demo_medical_loader():\n",
        "    \"\"\"D√©monstration du chargement de donn√©es m√©dicales\"\"\"\n",
        "\n",
        "    loader = MedicalDataLoader(\"./medical_data\")\n",
        "\n",
        "    loader.scan_medical_files()\n",
        "\n",
        "    if loader.dicom_data or loader.nifti_data:\n",
        "        loader.visualize_medical_data()\n",
        "\n",
        "        if loader.dicom_data:\n",
        "            print(\"\\n\" + \"=\"*50)\n",
        "            print(\" EXEMPLE DICOM D√âTAILL√â\")\n",
        "            print(\"=\"*50)\n",
        "\n",
        "            sample_dicom = loader.dicom_data[0][0]\n",
        "            image, metadata = loader.load_dicom_image(sample_dicom)\n",
        "\n",
        "            if image is not None:\n",
        "                print(f\" Dimensions image: {image.shape}\")\n",
        "                print(f\" Plage valeurs: [{np.min(image):.3f}, {np.max(image):.3f}]\")\n",
        "                print(f\"  M√©tadonn√©es: {metadata}\")\n",
        "\n",
        "        if loader.nifti_data:\n",
        "            print(\"\\n\" + \"=\"*50)\n",
        "            print(\" EXEMPLE NIFTI D√âTAILL√â\")\n",
        "            print(\"=\"*50)\n",
        "\n",
        "            sample_nifti = loader.nifti_data[0][0]\n",
        "            volume, metadata = loader.load_nifti_volume(sample_nifti)\n",
        "\n",
        "            if volume is not None:\n",
        "                print(f\" Dimensions volume: {volume.shape}\")\n",
        "                print(f\" Type donn√©es: {volume.dtype}\")\n",
        "                print(f\" Taille voxels: {metadata['voxel_sizes']}\")\n",
        "\n",
        "    else:\n",
        "        print(\" Aucune donn√©e m√©dicale trouv√©e.\")\n",
        "        print(\" Structure attendue:\")\n",
        "        print(\"   medical_data/\")\n",
        "        print(\"   ‚îú‚îÄ‚îÄ DICOM/\")\n",
        "        print(\"   ‚îÇ   ‚îú‚îÄ‚îÄ normal/*.dcm\")\n",
        "        print(\"   ‚îÇ   ‚îî‚îÄ‚îÄ pathological/*.dcm\")\n",
        "        print(\"   ‚îî‚îÄ‚îÄ NIFTI/\")\n",
        "        print(\"       ‚îú‚îÄ‚îÄ images/*.nii.gz\")\n",
        "        print(\"       ‚îî‚îÄ‚îÄ masks/*.nii.gz\")\n",
        "def create_sample_medical_data():\n",
        "    \"\"\"Cr√©e des donn√©es m√©dicales sample pour tester\"\"\"\n",
        "    sample_dir = \"./medical_data_sample\"\n",
        "    os.makedirs(sample_dir, exist_ok=True)\n",
        "\n",
        "    print(\" Cr√©ation de donn√©es sample...\")\n",
        "\n",
        "\n",
        "    print(f\" R√©pertoire sample cr√©√©: {sample_dir}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    create_sample_medical_data()\n",
        "\n",
        "    demo_medical_loader()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtnoZoQ9EsmQ",
        "outputId": "8b47f086-beae-4916-cffb-d7e7f05dd314"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " pydicom import√© avec succ√®s\n",
            " nibabel import√© avec succ√®s\n",
            " Cr√©ation de donn√©es sample...\n",
            " R√©pertoire sample cr√©√©: ./medical_data_sample\n",
            " Scan des fichiers m√©dicaux...\n",
            " Fichiers DICOM trouv√©s: 0\n",
            " Fichiers NIFTI trouv√©s: 0\n",
            " Aucune donn√©e m√©dicale trouv√©e.\n",
            " Structure attendue:\n",
            "   medical_data/\n",
            "   ‚îú‚îÄ‚îÄ DICOM/\n",
            "   ‚îÇ   ‚îú‚îÄ‚îÄ normal/*.dcm\n",
            "   ‚îÇ   ‚îî‚îÄ‚îÄ pathological/*.dcm\n",
            "   ‚îî‚îÄ‚îÄ NIFTI/\n",
            "       ‚îú‚îÄ‚îÄ images/*.nii.gz\n",
            "       ‚îî‚îÄ‚îÄ masks/*.nii.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_public_medical_datasets():\n",
        "    \"\"\"Liste des datasets m√©dicaux publics\"\"\"\n",
        "    datasets = {\n",
        "        'DICOM': [\n",
        "            'COVID-19 Chest X-Ray (Kaggle)',\n",
        "            'RSNA Pneumonia Detection',\n",
        "            'ChestX-ray8 (NIH)'\n",
        "        ],\n",
        "        'NIFTI': [\n",
        "            'BraTS (Brain Tumor Segmentation)',\n",
        "            'LiTS (Liver Tumor Segmentation)',\n",
        "            'MSD (Medical Segmentation Decathlon)'\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    print(\" Datasets m√©dicaux publics recommand√©s:\")\n",
        "    for modality, ds_list in datasets.items():\n",
        "        print(f\"\\n {modality}:\")\n",
        "        for ds in ds_list:\n",
        "            print(f\"   ‚Ä¢ {ds}\")\n",
        "\n",
        "    return datasets\n",
        "\n",
        "get_public_medical_datasets()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJIgBz47GuNl",
        "outputId": "3767f42c-a5e3-48e4-ef69-b7d518a655d9"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Datasets m√©dicaux publics recommand√©s:\n",
            "\n",
            " DICOM:\n",
            "   ‚Ä¢ COVID-19 Chest X-Ray (Kaggle)\n",
            "   ‚Ä¢ RSNA Pneumonia Detection\n",
            "   ‚Ä¢ ChestX-ray8 (NIH)\n",
            "\n",
            " NIFTI:\n",
            "   ‚Ä¢ BraTS (Brain Tumor Segmentation)\n",
            "   ‚Ä¢ LiTS (Liver Tumor Segmentation)\n",
            "   ‚Ä¢ MSD (Medical Segmentation Decathlon)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'DICOM': ['COVID-19 Chest X-Ray (Kaggle)',\n",
              "  'RSNA Pneumonia Detection',\n",
              "  'ChestX-ray8 (NIH)'],\n",
              " 'NIFTI': ['BraTS (Brain Tumor Segmentation)',\n",
              "  'LiTS (Liver Tumor Segmentation)',\n",
              "  'MSD (Medical Segmentation Decathlon)']}"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d tawsifurrahman/covid19-radiography-database"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mBiQlXXG0_O",
        "outputId": "66ee9fab-7272-46ca-b59e-e995ae7419b2"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/tawsifurrahman/covid19-radiography-database\n",
            "License(s): copyright-authors\n",
            "Downloading covid19-radiography-database.zip to /content\n",
            " 99% 773M/778M [00:09<00:00, 85.2MB/s]\n",
            "100% 778M/778M [00:09<00:00, 89.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "demo_medical_loader()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHYxwAoeKEvC",
        "outputId": "8314129b-01c2-453a-8bd5-1054d1d87896"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Scan des fichiers m√©dicaux...\n",
            " Fichiers DICOM trouv√©s: 0\n",
            " Fichiers NIFTI trouv√©s: 0\n",
            " Aucune donn√©e m√©dicale trouv√©e.\n",
            " Structure attendue:\n",
            "   medical_data/\n",
            "   ‚îú‚îÄ‚îÄ DICOM/\n",
            "   ‚îÇ   ‚îú‚îÄ‚îÄ normal/*.dcm\n",
            "   ‚îÇ   ‚îî‚îÄ‚îÄ pathological/*.dcm\n",
            "   ‚îî‚îÄ‚îÄ NIFTI/\n",
            "       ‚îú‚îÄ‚îÄ images/*.nii.gz\n",
            "       ‚îî‚îÄ‚îÄ masks/*.nii.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install kaggle --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMsOA2uwLDeZ",
        "outputId": "ac6b93c4-0166-4da1-dca2-bfa43096f2b6"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.12/dist-packages (1.8.2)\n",
            "Requirement already satisfied: black>=24.10.0 in /usr/local/lib/python3.12/dist-packages (from kaggle) (25.11.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.12/dist-packages (from kaggle) (6.3.0)\n",
            "Requirement already satisfied: kagglesdk in /usr/local/lib/python3.12/dist-packages (from kaggle) (0.1.13)\n",
            "Requirement already satisfied: mypy>=1.15.0 in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.18.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.12/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.32.4)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: types-requests in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.32.4.20250913)\n",
            "Requirement already satisfied: types-tqdm in /usr/local/lib/python3.12/dist-packages (from kaggle) (4.67.0.20250809)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.5.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from black>=24.10.0->kaggle) (8.3.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from black>=24.10.0->kaggle) (1.1.0)\n",
            "Requirement already satisfied: packaging>=22.0 in /usr/local/lib/python3.12/dist-packages (from black>=24.10.0->kaggle) (25.0)\n",
            "Requirement already satisfied: pathspec>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from black>=24.10.0->kaggle) (0.12.1)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.12/dist-packages (from black>=24.10.0->kaggle) (4.5.0)\n",
            "Requirement already satisfied: pytokens>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from black>=24.10.0->kaggle) (0.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from mypy>=1.15.0->kaggle) (4.15.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.12/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->kaggle) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->kaggle) (3.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->kaggle) (2025.11.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "kaggle_path = os.path.expanduser('~/.kaggle/kaggle.json')\n",
        "if not os.path.exists(kaggle_path):\n",
        "    print(\" kaggle.json non trouv√©. Suivez les instructions de configuration.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zjdC-5rLLRD",
        "outputId": "ed6a61df-ca60-4f01-934e-97f947213a32"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " kaggle.json non trouv√©. Suivez les instructions de configuration.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0bc41b3"
      },
      "source": [
        "# Create the .kaggle directory if it doesn't exist\n",
        "!mkdir -p ~/.kaggle\n"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf250e37",
        "outputId": "eafc531e-c0dc-462a-e9b7-f91d49f86bba"
      },
      "source": [
        "# Move the uploaded kaggle.json to the .kaggle directory\n",
        "!mv kaggle.json ~/.kaggle/\n"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mv: cannot stat 'kaggle.json': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cc4627a",
        "outputId": "e05e361b-ce3a-4a49-ef02-cd126f65ec76"
      },
      "source": [
        "# Set appropriate permissions for the kaggle.json file\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "print(\"Kaggle API key configured successfully!\")\n"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chmod: cannot access '/root/.kaggle/kaggle.json': No such file or directory\n",
            "Kaggle API key configured successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6d391f9",
        "outputId": "2f513fec-a122-4417-f348-d8f4c3d54c6f"
      },
      "source": [
        "# Verify the configuration by trying to list Kaggle datasets\n",
        "!kaggle datasets list -s covid19\n"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Could not find kaggle.json. Make sure it's located in /root/.kaggle. Or use the environment method. See setup instructions at https://github.com/Kaggle/kaggle-api/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17395847",
        "outputId": "495146cc-ec19-4dd6-e129-5b213a74da39"
      },
      "source": [
        "# Move the uploaded kaggle.json to the .kaggle directory\n",
        "!mv kaggle.json ~/.kaggle/"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mv: cannot stat 'kaggle.json': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7457abc1",
        "outputId": "13dbab11-8338-44b8-92b1-84d2690b4aa1"
      },
      "source": [
        "# Set appropriate permissions for the kaggle.json file\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "print(\"Kaggle API key configured successfully!\")"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chmod: cannot access '/root/.kaggle/kaggle.json': No such file or directory\n",
            "Kaggle API key configured successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e89bc0c3",
        "outputId": "671b3f23-3d31-4741-82d6-396b1d3d85c3"
      },
      "source": [
        "# Verify the configuration by trying to list Kaggle datasets\n",
        "!kaggle datasets list -s covid19"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Could not find kaggle.json. Make sure it's located in /root/.kaggle. Or use the environment method. See setup instructions at https://github.com/Kaggle/kaggle-api/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e13fce38",
        "outputId": "cbfbb2af-2d0d-4a08-90a3-2d0a506898b5"
      },
      "source": [
        "# Move the uploaded kaggle.json to the .kaggle directory\n",
        "!mv kaggle.json ~/.kaggle/"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mv: cannot stat 'kaggle.json': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6a417f2",
        "outputId": "bf290bee-14ae-492e-9ea7-40511d0e855e"
      },
      "source": [
        "# Set appropriate permissions for the kaggle.json file\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "print(\"Kaggle API key configured successfully!\")"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chmod: cannot access '/root/.kaggle/kaggle.json': No such file or directory\n",
            "Kaggle API key configured successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5d6c40cc",
        "outputId": "f5e7001c-ee43-4446-ff33-1d877c800556"
      },
      "source": [
        "# Verify the configuration by trying to list Kaggle datasets\n",
        "!kaggle datasets list -s covid19"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Could not find kaggle.json. Make sure it's located in /root/.kaggle. Or use the environment method. See setup instructions at https://github.com/Kaggle/kaggle-api/\n"
          ]
        }
      ]
    }
  ]
}